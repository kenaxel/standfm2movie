import { NextRequest, NextResponse } from 'next/server'
import { VideoGenerationRequest, VideoGenerationResult, VideoAsset, TranscriptSegment } from '@/types'
import fs from 'fs'
import path from 'path'
import os from 'os'
// FFmpegã‚’ä½¿ç”¨ã—ãŸå‹•ç”»ç”Ÿæˆã®ãŸã‚ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import ffmpeg from 'fluent-ffmpeg'
import OpenAI from 'openai'
import { searchVideos } from '@/lib/pexels'
import { searchPhotos } from '@/lib/unsplash'

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
})

// æ–‡å­—èµ·ã“ã—çµæœã‚’åˆ†æã—ã¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã‚·ãƒ¼ãƒ³ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°
async function analyzeTranscriptForKeywords(transcript: TranscriptSegment[]): Promise<{
  keywords: string[]
  scenes: Array<{
    startTime: number
    endTime: number
    keywords: string[]
    emotion: string
    visualConcepts: string[]
  }>
}> {
  try {
    const fullText = transcript.map(segment => segment.text).join(' ')
    
    const prompt = `ä»¥ä¸‹ã®éŸ³å£°æ–‡å­—èµ·ã“ã—çµæœã‚’åˆ†æã—ã¦ã€å‹•ç”»ã«é©ã—ãŸç”»åƒãƒ»å‹•ç”»ç´ æã‚’æ¤œç´¢ã™ã‚‹ãŸã‚ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã‚·ãƒ¼ãƒ³æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

æ–‡å­—èµ·ã“ã—å†…å®¹:
${fullText}

ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„:
{
  "keywords": ["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2", ...],
  "scenes": [
    {
      "startTime": 0,
      "endTime": 30,
      "keywords": ["ã‚·ãƒ¼ãƒ³å›ºæœ‰ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"],
      "emotion": "æ„Ÿæƒ…ï¼ˆhappy, sad, excited, calm, serious ãªã©ï¼‰",
      "visualConcepts": ["è¦–è¦šçš„ã‚³ãƒ³ã‚»ãƒ—ãƒˆï¼ˆnature, business, technology, people ãªã©ï¼‰"]
    }
  ]
}

æ³¨æ„:
- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯è‹±èªã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆç”»åƒæ¤œç´¢APIç”¨ï¼‰
- æ„Ÿæƒ…ã¯è‹±èªã®å˜èªã§è¡¨ç¾ã—ã¦ãã ã•ã„
- è¦–è¦šçš„ã‚³ãƒ³ã‚»ãƒ—ãƒˆã¯å…·ä½“çš„ã§æ¤œç´¢ã—ã‚„ã™ã„ã‚‚ã®ã«ã—ã¦ãã ã•ã„
- ã‚·ãƒ¼ãƒ³ã¯30ç§’ç¨‹åº¦ã®é•·ã•ã«åˆ†å‰²ã—ã¦ãã ã•ã„`

    const response = await openai.chat.completions.create({
      model: 'gpt-4',
      messages: [{ role: 'user', content: prompt }],
      temperature: 0.3
    })

    const analysisResult = JSON.parse(response.choices[0].message.content || '{"keywords": [], "scenes": []}')
    console.log('æ–‡å­—èµ·ã“ã—åˆ†æçµæœ:', analysisResult)
    
    return analysisResult
  } catch (error) {
    console.error('æ–‡å­—èµ·ã“ã—åˆ†æã‚¨ãƒ©ãƒ¼:', error)
    // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿”ã™
    return {
      keywords: ['business', 'presentation', 'meeting'],
      scenes: [{
        startTime: 0,
        endTime: transcript.reduce((max, segment) => Math.max(max, segment.endTime), 60),
        keywords: ['business'],
        emotion: 'neutral',
        visualConcepts: ['office', 'professional']
      }]
    }
  }
}

// ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åŸºã¥ã„ã¦ç”»åƒãƒ»å‹•ç”»ç´ æã‚’æ¤œç´¢ã™ã‚‹é–¢æ•°
async function searchMediaAssets(keywords: string[], scenes: any[]): Promise<VideoAsset[]> {
  try {
    const assets: VideoAsset[] = []
    
    // å„ã‚·ãƒ¼ãƒ³ã«å¯¾ã—ã¦ç´ æã‚’æ¤œç´¢
    for (const scene of scenes) {
      const sceneKeywords = [...scene.keywords, ...scene.visualConcepts]
      const searchQuery = sceneKeywords.join(' ')
      
      console.log(`ã‚·ãƒ¼ãƒ³ ${scene.startTime}-${scene.endTime}s ã®æ¤œç´¢ã‚¯ã‚¨ãƒª:`, searchQuery)
      
      try {
        // Pexelsã‹ã‚‰å‹•ç”»ã‚’æ¤œç´¢
        const videos = await searchVideos(searchQuery, 3)
        for (const video of videos) {
          assets.push({
            type: 'video',
            url: video.url,
            duration: video.duration || 10,
            startTime: scene.startTime,
            endTime: scene.endTime
          })
        }
        
        // Unsplashã‹ã‚‰ç”»åƒã‚’æ¤œç´¢
        const images = await searchPhotos(searchQuery, 5)
        for (const image of images) {
          assets.push({
            type: 'image',
            url: image.url,
            duration: 5, // ç”»åƒã®è¡¨ç¤ºæ™‚é–“
            startTime: scene.startTime,
            endTime: scene.endTime
          })
        }
        
      } catch (searchError) {
        console.error(`ã‚·ãƒ¼ãƒ³ ${scene.startTime}-${scene.endTime}s ã®æ¤œç´¢ã‚¨ãƒ©ãƒ¼:`, searchError)
        
        // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç”»åƒã‚’è¿½åŠ 
        assets.push({
          type: 'image',
          url: 'https://via.placeholder.com/1280x720/0066cc/ffffff?text=Scene+' + Math.floor(scene.startTime),
          duration: scene.endTime - scene.startTime,
          startTime: scene.startTime,
          endTime: scene.endTime
        })
      }
    }
    
    console.log(`æ¤œç´¢å®Œäº†: ${assets.length}å€‹ã®ç´ æã‚’å–å¾—`)
    return assets
    
  } catch (error) {
    console.error('ç´ ææ¤œç´¢ã‚¨ãƒ©ãƒ¼:', error)
    return []
  }
}

// éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®é•·ã•ã‚’å–å¾—ã™ã‚‹é–¢æ•°
async function getAudioDuration(audioPath: string): Promise<number> {
  return new Promise((resolve, reject) => {
    const ffmpeg = require('fluent-ffmpeg')
    
    ffmpeg.ffprobe(audioPath, (err: any, metadata: any) => {
      if (err) {
        console.error('FFprobe error:', err)
        reject(err)
        return
      }
      
      const duration = metadata.format.duration
      console.log('Audio duration detected:', duration, 'seconds')
      resolve(duration)
    })
  })
}

// éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†é–¢æ•°
async function processAudioFile(audioInput: any, tempDir: string): Promise<string | null> {
  if (!audioInput) return null
  
  try {
    let audioPath: string | null = null
    
    // æ–‡å­—åˆ—ã®å ´åˆã¯éŸ³å£°åˆæˆã‚’è¡Œã†
    if (typeof audioInput === 'string') {
      console.log('Text input detected, generating speech...')
      audioPath = path.join(tempDir, 'synthesized_audio.mp3')
      
      // ç°¡å˜ãªéŸ³å£°åˆæˆï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯å¤–éƒ¨APIã‚’ä½¿ç”¨ï¼‰
      // ã“ã“ã§ã¯ç„¡éŸ³ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
      const { spawn } = require('child_process')
      
      return new Promise((resolve, reject) => {
        // ãƒ†ã‚­ã‚¹ãƒˆã‚’éŸ³å£°ã«å¤‰æ›ï¼ˆç°¡æ˜“ç‰ˆï¼šãƒ“ãƒ¼ãƒ—éŸ³ã‚’ç”Ÿæˆï¼‰
        const duration = 10 // 10ç§’
        const ffmpeg = spawn('ffmpeg', [
          '-f', 'lavfi',
          '-i', `sine=frequency=440:duration=${duration}`,
          '-c:a', 'mp3',
          '-ar', '44100',
          '-ac', '2',
          '-y',
          audioPath
        ])
        
        ffmpeg.on('close', (code: number) => {
          if (code === 0) {
            console.log('Synthesized audio created:', audioPath)
            resolve(audioPath)
          } else {
            console.error('Failed to create synthesized audio')
            resolve(null)
          }
        })
        
        ffmpeg.on('error', (err: Error) => {
          console.error('FFmpeg error:', err)
          resolve(null)
        })
      })
    } else if (audioInput.type === 'file' && audioInput.data) {
      // Base64ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
      const audioBuffer = Buffer.from(audioInput.data, 'base64')
      const timestamp = Date.now()
      const randomId = Math.random().toString(36).substring(2, 15)
      audioPath = path.join(tempDir, `audio_${timestamp}_${randomId}.${audioInput.format || 'mp3'}`)
      await fs.promises.writeFile(audioPath, audioBuffer)
      console.log('Audio file saved with unique name:', audioPath)
    } else if (audioInput.type === 'url' && audioInput.source) {
      // URLã‹ã‚‰éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
      const timestamp = Date.now()
      const randomId = Math.random().toString(36).substring(2, 15)
      audioPath = path.join(tempDir, `audio_${timestamp}_${randomId}.mp3`)
      console.log('Audio URL provided:', audioInput.source, 'Target file:', audioPath)
      
      try {
        let audioUrl = audioInput.source
        
        // Stand.FMã®URLã®å ´åˆã€å®Ÿéš›ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«URLã‚’å–å¾—
        if (audioUrl.includes('stand.fm')) {
          console.log('Stand.FM URL detected, extracting audio URL...')
          
          try {
            // Stand.FMã®APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’è©¦ã™
            const episodeId = audioUrl.match(/episodes\/([a-f0-9]+)/)?.[1]
            if (episodeId) {
              console.log('Episode ID found:', episodeId)
              
              // Stand.FM APIã‚’è©¦ã™
              const apiUrl = `https://stand.fm/api/episodes/${episodeId}`
              const apiResponse = await fetch(apiUrl)
              if (apiResponse.ok) {
                const apiData = await apiResponse.json()
                if (apiData.audio_url) {
                  audioUrl = apiData.audio_url
                  console.log('Audio URL from API:', audioUrl)
                } else if (apiData.episode && apiData.episode.audio_url) {
                  audioUrl = apiData.episode.audio_url
                  console.log('Audio URL from episode data:', audioUrl)
                }
              }
            }
            
            // APIã§å–å¾—ã§ããªã„å ´åˆã¯HTMLãƒšãƒ¼ã‚¸ã‹ã‚‰æŠ½å‡º
            if (audioUrl.includes('stand.fm')) {
              const pageResponse = await fetch(audioUrl)
              const pageHtml = await pageResponse.text()
              
              // è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
              const patterns = [
                /"audio_url"\s*:\s*"([^"]+)"/,
                /"audioUrl"\s*:\s*"([^"]+)"/,
                /"src"\s*:\s*"([^"]*\.mp3[^"]*)"/,
                /https:\/\/[^"\s]*\.mp3/g,
                /https:\/\/[^"\s]*audio[^"\s]*\.(mp3|m4a)/g,
                /__NEXT_DATA__[^<]*"audio_url":"([^"]+)"/,
                /window\.__INITIAL_STATE__[^<]*"audio_url":"([^"]+)"/
              ]
              
              for (const pattern of patterns) {
                const match = pageHtml.match(pattern)
                if (match && match[1]) {
                  audioUrl = match[1]
                  console.log('Audio URL extracted with pattern:', audioUrl)
                  break
                } else if (match && match[0] && match[0].includes('.mp3')) {
                  audioUrl = match[0]
                  console.log('Direct MP3 URL found:', audioUrl)
                  break
                }
              }
              
              // å…ƒã®Stand.FMãƒšãƒ¼ã‚¸URLã¨åŒã˜ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆéŸ³å£°URLãŒæŠ½å‡ºã§ãã¦ã„ãªã„å ´åˆï¼‰
              if (audioUrl === audioInput.source) {
                console.log('Could not extract audio URL from Stand.FM page')
                console.log('Page HTML length:', pageHtml.length)
                // HTMLã®ä¸€éƒ¨ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                const htmlSnippet = pageHtml.substring(0, 1000)
                console.log('HTML snippet:', htmlSnippet)
                return null
              }
            }
          } catch (extractError) {
            console.error('Error extracting Stand.FM audio URL:', extractError)
            return null
          }
        }
        
        const response = await fetch(audioUrl)
        if (!response.ok) {
          throw new Error(`HTTP error! status: ${response.status}`)
        }
        const audioBuffer = await response.arrayBuffer()
        await fs.promises.writeFile(audioPath, Buffer.from(audioBuffer))
        console.log('Audio downloaded and saved:', audioPath)
      } catch (downloadError) {
        console.error('Failed to download audio from URL:', downloadError)
        // URLãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸå ´åˆã¯ã€éŸ³å£°ãªã—ã§ç¶šè¡Œ
        return null
      }
    }
    
    return audioPath
  } catch (error) {
    console.error('Audio processing error:', error)
    return null
  }
}

// ç´ æã‚’ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã«é…ç½®ã™ã‚‹é–¢æ•°
async function arrangeAssetsOnTimeline(assets: VideoAsset[], totalDuration: number): Promise<{
  timeline: Array<{
    asset: VideoAsset
    startTime: number
    endTime: number
    transitionType: 'fade' | 'slide' | 'zoom'
  }>
  overlayAssets: VideoAsset[]
}> {
  try {
    const timeline: Array<{
      asset: VideoAsset
      startTime: number
      endTime: number
      transitionType: 'fade' | 'slide' | 'zoom'
    }> = []
    
    const overlayAssets: VideoAsset[] = []
    
    // ç´ æã‚’æ™‚é–“é †ã«ã‚½ãƒ¼ãƒˆ
    const sortedAssets = assets.sort((a, b) => a.startTime - b.startTime)
    
    let currentTime = 0
    
    for (const asset of sortedAssets) {
      // ç´ æã®é…ç½®æ™‚é–“ã‚’èª¿æ•´
      const startTime = Math.max(currentTime, asset.startTime)
      const duration = Math.min(asset.duration, asset.endTime - asset.startTime)
      const endTime = startTime + duration
      
      // ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã«è¿½åŠ 
      timeline.push({
        asset,
        startTime,
        endTime,
        transitionType: getTransitionType(asset.type)
      })
      
      currentTime = endTime
      
      // ç·æ™‚é–“ã‚’è¶…ãˆãªã„ã‚ˆã†ã«ãƒã‚§ãƒƒã‚¯
      if (currentTime >= totalDuration) {
        break
      }
    }
    
    // æ™‚é–“ã®éš™é–“ã‚’åŸ‹ã‚ã‚‹ãŸã‚ã®ãƒ•ã‚£ãƒ©ãƒ¼ç´ æã‚’è¿½åŠ 
    const filledTimeline = await fillTimelineGaps(timeline, totalDuration)
    
    console.log(`ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³é…ç½®å®Œäº†: ${filledTimeline.length}å€‹ã®ç´ æã‚’é…ç½®`)
    
    return {
      timeline: filledTimeline,
      overlayAssets
    }
    
  } catch (error) {
    console.error('ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³é…ç½®ã‚¨ãƒ©ãƒ¼:', error)
    return { timeline: [], overlayAssets: [] }
  }
}

// ãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—ã‚’æ±ºå®šã™ã‚‹é–¢æ•°
function getTransitionType(assetType: 'image' | 'video'): 'fade' | 'slide' | 'zoom' {
  const transitions: Array<'fade' | 'slide' | 'zoom'> = ['fade', 'slide', 'zoom']
  return transitions[Math.floor(Math.random() * transitions.length)]
}

// ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã®éš™é–“ã‚’åŸ‹ã‚ã‚‹é–¢æ•°
async function fillTimelineGaps(timeline: any[], totalDuration: number): Promise<any[]> {
  const filledTimeline = [...timeline]
  
  // æœ€åˆã®ç´ æã®å‰ã«éš™é–“ãŒã‚ã‚‹å ´åˆ
  if (timeline.length > 0 && timeline[0].startTime > 0) {
    filledTimeline.unshift({
      asset: {
        type: 'image',
        url: 'https://via.placeholder.com/1920x1080/4f46e5/ffffff?text=Opening',
        duration: timeline[0].startTime,
        startTime: 0,
        endTime: timeline[0].startTime
      },
      startTime: 0,
      endTime: timeline[0].startTime,
      transitionType: 'fade'
    })
  }
  
  // ç´ æé–“ã®éš™é–“ã‚’åŸ‹ã‚ã‚‹
  for (let i = 0; i < timeline.length - 1; i++) {
    const currentEnd = timeline[i].endTime
    const nextStart = timeline[i + 1].startTime
    
    if (nextStart > currentEnd) {
      const gapDuration = nextStart - currentEnd
      filledTimeline.splice(i + 1, 0, {
        asset: {
          type: 'image',
          url: `https://via.placeholder.com/1920x1080/6366f1/ffffff?text=Transition+${i + 1}`,
          duration: gapDuration,
          startTime: currentEnd,
          endTime: nextStart
        },
        startTime: currentEnd,
        endTime: nextStart,
        transitionType: 'fade'
      })
    }
  }
  
  // æœ€å¾Œã®ç´ æã®å¾Œã«éš™é–“ãŒã‚ã‚‹å ´åˆ
  const lastAsset = timeline[timeline.length - 1]
  if (lastAsset && lastAsset.endTime < totalDuration) {
    filledTimeline.push({
      asset: {
        type: 'image',
        url: 'https://via.placeholder.com/1920x1080/8b5cf6/ffffff?text=Ending',
        duration: totalDuration - lastAsset.endTime,
        startTime: lastAsset.endTime,
        endTime: totalDuration
      },
      startTime: lastAsset.endTime,
      endTime: totalDuration,
      transitionType: 'fade'
    })
  }
  
  return filledTimeline
}

// ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ãƒ™ãƒ¼ã‚¹ã®å‹•ç”»ç”Ÿæˆé–¢æ•°
async function generateTimelineBasedVideo({
  timeline,
  audioPath,
  outputPath,
  settings,
  tempDir
}: {
  timeline: any[]
  audioPath: string | null
  outputPath: string
  settings: any
  tempDir: string
}): Promise<string> {
  try {
    console.log(`ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³å‹•ç”»ç”Ÿæˆ: ${timeline.length}å€‹ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ`)
    
    // å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆç”¨ã®å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
    const segmentPaths: string[] = []
    
    for (let i = 0; i < timeline.length; i++) {
      const segment = timeline[i]
      const segmentPath = path.join(tempDir, `segment_${i}.mp4`)
      
      console.log(`ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ ${i + 1}/${timeline.length} ç”Ÿæˆä¸­:`, segment.asset.url)
      
      // ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå‹•ç”»ã‚’ç”Ÿæˆ
      await generateSegmentVideo({
        asset: segment.asset,
        startTime: segment.startTime,
        endTime: segment.endTime,
        transitionType: segment.transitionType,
        outputPath: segmentPath,
        settings,
        tempDir
      })
      
      segmentPaths.push(segmentPath)
    }
    
    // ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’çµåˆ
    console.log('ã‚»ã‚°ãƒ¡ãƒ³ãƒˆçµåˆé–‹å§‹')
    await concatenateSegments(segmentPaths, outputPath, audioPath, settings)
    
    console.log('ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³å‹•ç”»ç”Ÿæˆå®Œäº†')
    return outputPath
    
  } catch (error) {
    console.error('ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³å‹•ç”»ç”Ÿæˆã‚¨ãƒ©ãƒ¼:', error)
    throw error
  }
}

// å€‹åˆ¥ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå‹•ç”»ç”Ÿæˆé–¢æ•°
async function generateSegmentVideo({
  asset,
  startTime,
  endTime,
  transitionType,
  outputPath,
  settings,
  tempDir
}: {
  asset: VideoAsset
  startTime: number
  endTime: number
  transitionType: string
  outputPath: string
  settings: any
  tempDir: string
}): Promise<void> {
  return new Promise((resolve, reject) => {
    const duration = endTime - startTime
    
    let ffmpegCommand = ffmpeg()
    
    if (asset.type === 'image') {
      // ç”»åƒã®å ´åˆ
      ffmpegCommand
        .input(asset.url)
        .inputOptions(['-loop 1', '-t', duration.toString()])
        .videoFilters([
          `scale=${settings.resolution?.width || 1920}:${settings.resolution?.height || 1080}:force_original_aspect_ratio=decrease`,
          `pad=${settings.resolution?.width || 1920}:${settings.resolution?.height || 1080}:(ow-iw)/2:(oh-ih)/2`,
          getTransitionFilter(transitionType, duration)
        ])
    } else {
      // å‹•ç”»ã®å ´åˆ
      ffmpegCommand
        .input(asset.url)
        .inputOptions(['-t', duration.toString()])
        .videoFilters([
          `scale=${settings.resolution?.width || 1920}:${settings.resolution?.height || 1080}:force_original_aspect_ratio=decrease`,
          `pad=${settings.resolution?.width || 1920}:${settings.resolution?.height || 1080}:(ow-iw)/2:(oh-ih)/2`,
          getTransitionFilter(transitionType, duration)
        ])
    }
    
    ffmpegCommand
      .videoCodec('libx264')
      .fps(settings.fps || 30)
      .outputOptions(['-pix_fmt yuv420p', '-preset fast', '-crf 28'])
      .output(outputPath)
      .on('end', () => resolve())
      .on('error', (err) => reject(err))
      .run()
  })
}

// ãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’å–å¾—ã™ã‚‹é–¢æ•°
function getTransitionFilter(transitionType: string, duration: number): string {
  switch (transitionType) {
    case 'fade':
      return `fade=in:0:30,fade=out:${Math.max(0, duration * 30 - 30)}:30`
    case 'slide':
      return `slide=direction=right:duration=1`
    case 'zoom':
      return `zoompan=z='1+0.002*on':d=1`
    default:
      return `fade=in:0:15,fade=out:${Math.max(0, duration * 30 - 15)}:15`
  }
}

// ã‚»ã‚°ãƒ¡ãƒ³ãƒˆçµåˆé–¢æ•°
async function concatenateSegments(
  segmentPaths: string[],
  outputPath: string,
  audioPath: string | null,
  settings: any
): Promise<void> {
  return new Promise((resolve, reject) => {
    const ffmpegCommand = ffmpeg()
    
    // å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å…¥åŠ›ã¨ã—ã¦è¿½åŠ 
    segmentPaths.forEach(segmentPath => {
      ffmpegCommand.input(segmentPath)
    })
    
    // éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã¯è¿½åŠ 
    if (audioPath && fs.existsSync(audioPath)) {
      ffmpegCommand.input(audioPath)
    }
    
    // ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰
    const filterComplex = segmentPaths.map((_, index) => `[${index}:v]`).join('') + `concat=n=${segmentPaths.length}:v=1:a=0[outv]`
    
    ffmpegCommand
      .complexFilter(filterComplex)
      .outputOptions(['-map [outv]'])
    
    // éŸ³å£°ãƒãƒƒãƒ”ãƒ³ã‚°
    if (audioPath && fs.existsSync(audioPath)) {
      ffmpegCommand.outputOptions([`-map ${segmentPaths.length}:a`, '-shortest'])
    }
    
    ffmpegCommand
      .videoCodec('libx264')
      .audioCodec('aac')
      .outputOptions(['-pix_fmt yuv420p', '-preset fast', '-crf 28'])
      .output(outputPath)
      .on('start', (commandLine) => {
        console.log('çµåˆã‚³ãƒãƒ³ãƒ‰:', commandLine)
      })
      .on('progress', (progress) => {
        console.log('çµåˆé€²è¡ŒçŠ¶æ³:', progress.percent + '%')
      })
      .on('end', () => {
        console.log('ã‚»ã‚°ãƒ¡ãƒ³ãƒˆçµåˆå®Œäº†')
        resolve()
      })
      .on('error', (err) => {
        console.error('çµåˆã‚¨ãƒ©ãƒ¼:', err)
        reject(err)
      })
      .run()
  })
}

// éŸ³å£°ã®æ–‡å­—èµ·ã“ã—é–¢æ•°ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãï¼‰
async function transcribeAudioWithTimestamps(audioPath: string): Promise<TranscriptSegment[]> {
  try {
    console.log('OpenAI Whisper APIã§æ–‡å­—èµ·ã“ã—é–‹å§‹:', audioPath)
    
    // OpenAI SDKã‚’ä½¿ç”¨ã—ã¦æ–‡å­—èµ·ã“ã—
    const transcription = await openai.audio.transcriptions.create({
      file: fs.createReadStream(audioPath),
      model: 'whisper-1',
      response_format: 'verbose_json',
      timestamp_granularities: ['segment']
    })
    
    console.log('Whisper APIçµæœ:', transcription)
    
    // ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›
    const segments: TranscriptSegment[] = []
    if (transcription.segments && Array.isArray(transcription.segments)) {
      for (const segment of transcription.segments) {
        segments.push({
          text: segment.text.trim(),
          startTime: segment.start,
          endTime: segment.end
        })
      }
    } else if (transcription.text) {
      // ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæƒ…å ±ãŒãªã„å ´åˆã¯å…¨ä½“ã‚’ä¸€ã¤ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¨ã—ã¦æ‰±ã†
      segments.push({
        text: transcription.text.trim(),
        startTime: 0,
        endTime: 60 // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
      })
    }
    
    console.log('æ–‡å­—èµ·ã“ã—å®Œäº†:', segments.length, 'å€‹ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ')
    return segments
    
  } catch (error) {
    console.error('æ–‡å­—èµ·ã“ã—ã‚¨ãƒ©ãƒ¼:', error)
    // ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ç©ºã®é…åˆ—ã‚’è¿”ã™
    return []
  }
}

// éŸ³å£°ã‚’å­—å¹•ã¨ã—ã¦è¡¨ç¤ºã™ã‚‹å‹•ç”»ç”Ÿæˆé–¢æ•°
async function generateSubtitleVideo({
  audioPath,
  audioInput,
  transcript,
  settings,
  tempDir,
  timeline,
  audioDuration
}: {
  audioPath: string
  audioInput: string
  transcript: TranscriptSegment[]
  settings: any
  tempDir: string
  timeline?: any
  audioDuration: number
}): Promise<string> {
  const timestamp = Date.now()
  const outputPath = path.join(tempDir, `output_${timestamp}.mp4`)
  
  console.log('Generating video with audio and subtitles')
  
  try {
    // å­—å¹•ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆSRTå½¢å¼ï¼‰ã‚’ç”Ÿæˆ
    const subtitlePath = path.join(tempDir, `subtitles_${timestamp}.srt`)
    let srtContent = ''
    
    console.log('å­—å¹•ç”Ÿæˆ - transcript:', transcript ? transcript.length : 'null', 'å€‹ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ')
    console.log('transcriptè©³ç´°:', transcript)
    if (transcript && transcript.length > 0) {
      // æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­—å¹•ã‚’ç”Ÿæˆ
      console.log('transcriptã‹ã‚‰å­—å¹•ã‚’ç”Ÿæˆ:', transcript)
      transcript.forEach((segment, index) => {
        const startTime = formatSRTTime(segment.startTime)
        const endTime = formatSRTTime(segment.endTime)
        srtContent += `${index + 1}\n${startTime} --> ${endTime}\n${segment.text}\n\n`
        console.log(`å­—å¹•ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ ${index + 1}: ${startTime} --> ${endTime} | ${segment.text}`)
      })
    } else {
      // éŸ³å£°å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¨ä½“ã®å­—å¹•ã¨ã—ã¦ä½¿ç”¨
      console.log('transcriptãŒç©ºã®ãŸã‚ã€audioInputã‹ã‚‰å­—å¹•ã‚’ç”Ÿæˆ')
      const text = typeof audioInput === 'string' ? audioInput : (audioInput as any).source
      
      // ãƒ†ã‚­ã‚¹ãƒˆã‚’20å€‹ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«åˆ†å‰²
      const words = text.split(' ')
      const wordsPerSegment = Math.ceil(words.length / 20)
      const segmentDuration = audioDuration / 20
      
      for (let i = 0; i < 20; i++) {
        const startTime = i * segmentDuration
        const endTime = (i + 1) * segmentDuration
        const segmentWords = words.slice(i * wordsPerSegment, (i + 1) * wordsPerSegment)
        const segmentText = segmentWords.join(' ')
        
        if (segmentText.trim()) {
          srtContent += `${i + 1}\n${formatSRTTime(startTime)} --> ${formatSRTTime(endTime)}\n${segmentText}\n\n`
        }
      }
    }
    
    await fs.promises.writeFile(subtitlePath, srtContent, 'utf-8')
    console.log('Subtitle file created:', subtitlePath)
    
    // èƒŒæ™¯ç”»åƒã‚’ç”Ÿæˆ
    const { createCanvas } = require('canvas')
    const canvas = createCanvas(settings.resolution?.width || 1920, settings.resolution?.height || 1080)
    const ctx = canvas.getContext('2d')
    
    // ã‚·ãƒ³ãƒ—ãƒ«ãªèƒŒæ™¯ï¼ˆã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
    const gradient = ctx.createLinearGradient(0, 0, 0, canvas.height)
    gradient.addColorStop(0, '#2c3e50')
    gradient.addColorStop(1, '#34495e')
    ctx.fillStyle = gradient
    ctx.fillRect(0, 0, canvas.width, canvas.height)
    
    // èƒŒæ™¯ç´ æã®æº–å‚™
    let backgroundInput = ''
    let inputOptions: string[] = []
    
    if (timeline && timeline.timeline && timeline.timeline.length > 0) {
      // å‹•ç”»ç´ æãŒã‚ã‚‹å ´åˆã¯æœ€åˆã®å‹•ç”»ã‚’ä½¿ç”¨
      const firstVideoAsset = timeline.timeline.find((item: any) => item.asset && item.asset.url)
      if (firstVideoAsset && firstVideoAsset.asset.url) {
        backgroundInput = firstVideoAsset.asset.url
        console.log('Using video asset as background:', backgroundInput)
      } else {
        // å‹•ç”»ç´ æãŒãªã„å ´åˆã¯é™æ­¢ç”»åƒã‚’ä½œæˆ
        const backgroundPath = path.join(tempDir, 'background.png')
        const buffer = canvas.toBuffer('image/png')
        await fs.promises.writeFile(backgroundPath, buffer)
        backgroundInput = backgroundPath
        inputOptions = ['-loop 1']
        console.log('Background image created:', backgroundPath)
      }
    } else {
      // timelineãŒãªã„å ´åˆã¯é™æ­¢ç”»åƒã‚’ä½œæˆ
      const backgroundPath = path.join(tempDir, 'background.png')
      const buffer = canvas.toBuffer('image/png')
      await fs.promises.writeFile(backgroundPath, buffer)
      backgroundInput = backgroundPath
      inputOptions = ['-loop 1']
      console.log('Background image created:', backgroundPath)
    }
    
    // FFmpegã§å‹•ç”»ç”Ÿæˆï¼ˆéŸ³å£° + èƒŒæ™¯ + å­—å¹•ï¼‰
    return new Promise((resolve, reject) => {
      let ffmpegCommand = ffmpeg()
        .input(backgroundInput)
      
      if (inputOptions.length > 0) {
        ffmpegCommand.inputOptions(inputOptions)
      }
      
      // éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ 
      if (audioPath && fs.existsSync(audioPath)) {
        console.log('Adding audio track:', audioPath)
        ffmpegCommand.input(audioPath)
      }
      
      // å­—å¹•ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’è¿½åŠ ï¼ˆforce_styleã‚’å‰Šé™¤ã—ã¦SRTã®æ™‚é–“æƒ…å ±ã‚’å°Šé‡ï¼‰
      const subtitleFilter = `subtitles=${subtitlePath.replace(/\\/g, '\\\\').replace(/:/g, '\\:')}`
      
      // å‹•ç”»ç´ æã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¨ãã†ã§ãªã„å ´åˆã§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’åˆ†ã‘ã‚‹
      const videoFilters = [`scale=${settings.resolution?.width || 1920}:${settings.resolution?.height || 1080}`]
      
      // é™æ­¢ç”»åƒã®å ´åˆã®ã¿å‹•çš„åŠ¹æœã‚’è¿½åŠ 
      if (inputOptions.includes('-loop 1')) {
        videoFilters.push(
          // å‹•çš„ãªè¦–è¦šåŠ¹æœã‚’è¿½åŠ 
          'zoompan=z=\'1+0.0005*on\':d=1:s=1920x1080',
          // è‰²ç›¸ã®å¾®å¦™ãªå¤‰åŒ–
          'hue=h=sin(2*PI*t/15)*20',
          // ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³åŠ¹æœ
          'fade=in:0:30'
        )
      }
      
      // å­—å¹•ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã¯å¸¸ã«è¿½åŠ 
      videoFilters.push(subtitleFilter)
      
      ffmpegCommand.videoFilters(videoFilters)
        .videoCodec('libx264')
        .fps(settings.fps || 30)
        .outputOptions(['-pix_fmt yuv420p', '-preset fast', '-crf 28'])
      
      // éŸ³å£°ãŒã‚ã‚‹å ´åˆã¯éŸ³å£°ã‚’çµ±åˆ
      if (audioPath && fs.existsSync(audioPath)) {
        console.log('Integrating audio with video')
        ffmpegCommand
          .audioCodec('aac')
          .audioBitrate('128k')
          .outputOptions(['-t', audioDuration.toString()])
      } else {
        console.log('No audio file, creating silent video')
        ffmpegCommand.outputOptions(['-t', audioDuration.toString()])
      }
      
      ffmpegCommand
        .output(outputPath)
        .on('start', (commandLine) => {
          console.log('FFmpeg command:', commandLine)
        })
        .on('progress', (progress) => {
          console.log('Processing: ' + progress.percent + '% done')
        })
        .on('end', () => {
          console.log('Subtitle video generation completed')
          resolve(outputPath)
        })
        .on('error', (err) => {
          console.error('FFmpeg error:', err)
          reject(err)
        })
        .run()
    })
  } catch (error) {
    console.error('Subtitle video generation error:', error)
    throw error
  }
}

// SRTæ™‚é–“ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆé–¢æ•°
function formatSRTTime(seconds: number): string {
  const hours = Math.floor(seconds / 3600)
  const minutes = Math.floor((seconds % 3600) / 60)
  const secs = Math.floor(seconds % 60)
  const milliseconds = Math.floor((seconds % 1) * 1000)
  
  return `${hours.toString().padStart(2, '0')}:${minutes.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')},${milliseconds.toString().padStart(3, '0')}`
}

// å‹•çš„ãªå‹•ç”»ç”Ÿæˆé–¢æ•°
async function generateSimpleVideo({
  audioInput,
  settings,
  outputPath,
  tempDir,
  timeline
}: {
  audioInput: any
  settings: any
  outputPath: string
  tempDir: string
  timeline?: any[]
}) {
  console.log('Generating animated video with Canvas and FFmpeg')
  
  try {
    // éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    const audioPath = await processAudioFile(audioInput, tempDir)
    console.log('Subtitle video - Processed audio path:', audioPath)
    
    // éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
    if (audioPath && fs.existsSync(audioPath)) {
      const stats = await fs.promises.stat(audioPath)
      console.log('Subtitle video - Audio file size:', stats.size, 'bytes')
    } else {
      console.log('Subtitle video - No audio file available or file does not exist')
     }
    
    // Canvas ã§é­…åŠ›çš„ãªèƒŒæ™¯ç”»åƒã‚’ç”Ÿæˆ
    const { createCanvas } = require('canvas')
    const canvas = createCanvas(settings.resolution?.width || 1920, settings.resolution?.height || 1080)
    const ctx = canvas.getContext('2d')
    
    // å‹•çš„ãªèƒŒæ™¯ã®ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    const gradient = ctx.createLinearGradient(0, 0, canvas.width, canvas.height)
    gradient.addColorStop(0, '#ff6b6b')
    gradient.addColorStop(0.3, '#4ecdc4')
    gradient.addColorStop(0.6, '#45b7d1')
    gradient.addColorStop(1, '#96ceb4')
    ctx.fillStyle = gradient
    ctx.fillRect(0, 0, canvas.width, canvas.height)
    
    // éŸ³å£°æ³¢å½¢é¢¨ã®è£…é£¾
    ctx.strokeStyle = 'rgba(255, 255, 255, 0.4)'
    ctx.lineWidth = 4
    for (let i = 0; i < 30; i++) {
      const x = (canvas.width / 30) * i + 30
      const baseHeight = canvas.height * 0.7
      const waveHeight = Math.sin(i * 0.3) * 60 + 80
      
      ctx.beginPath()
      ctx.moveTo(x, baseHeight)
      ctx.lineTo(x, baseHeight - waveHeight)
      ctx.stroke()
    }
    
    // ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¤ãƒˆãƒ«
    ctx.shadowColor = '#ffffff'
    ctx.shadowBlur = 10
    ctx.fillStyle = 'white'
    ctx.font = 'bold 64px Arial'
    ctx.textAlign = 'center'
    ctx.textBaseline = 'middle'
    ctx.fillText('éŸ³å£°ã‹ã‚‰å‹•ç”»ç”Ÿæˆ', canvas.width / 2, canvas.height / 2 - 50)
    
    // éŸ³å£°ãƒˆãƒ©ãƒƒã‚¯æƒ…å ±
    ctx.shadowBlur = 0
    if (audioPath) {
      ctx.fillStyle = 'rgba(251, 191, 36, 0.8)'
      ctx.font = '32px Arial'
      ctx.textAlign = 'center'
      ctx.fillText('ğŸµ éŸ³å£°ãƒˆãƒ©ãƒƒã‚¯ä»˜ã', canvas.width / 2, canvas.height / 2 + 50)
    } else {
      ctx.fillStyle = 'rgba(148, 163, 184, 0.8)'
      ctx.font = '24px Arial'
      ctx.textAlign = 'center'
      ctx.fillText('éŸ³å£°ãªã—', canvas.width / 2, canvas.height / 2 + 50)
    }
    
    // ç”Ÿæˆæ™‚åˆ»
    const now = new Date()
    ctx.fillStyle = 'rgba(226, 232, 240, 0.6)'
    ctx.font = '20px Arial'
    ctx.textAlign = 'right'
    ctx.fillText(`ç”Ÿæˆæ™‚åˆ»: ${now.toLocaleString('ja-JP')}`, canvas.width - 50, 50)
    
    // é™æ­¢ç”»ã‚’ä¿å­˜
    const imagePath = path.join(tempDir, 'background.png')
    const buffer = canvas.toBuffer('image/png')
    await fs.promises.writeFile(imagePath, buffer)
    
    console.log('Background image created:', imagePath)
    
    // ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ãƒ™ãƒ¼ã‚¹ã®å‹•ç”»ç”Ÿæˆ
    if (timeline && timeline.length > 0) {
      console.log('ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ãƒ™ãƒ¼ã‚¹ã®å‹•ç”»ç”Ÿæˆé–‹å§‹')
      return await generateTimelineBasedVideo({
        timeline,
        audioPath,
        outputPath,
        settings,
        tempDir
      })
    }
    
    // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é™æ­¢ç”»ãƒ™ãƒ¼ã‚¹ã®å‹•ç”»ç”Ÿæˆ
    return new Promise((resolve, reject) => {
      const ffmpegCommand = ffmpeg()
        .input(imagePath)
        .inputOptions(['-loop 1', '-t', (settings.duration || 60).toString()])
        .videoCodec('libx264')
        .fps(settings.fps || 30)
        .videoFilters([
          // ã‚†ã£ãã‚Šã¨ã—ãŸã‚ºãƒ¼ãƒ ã‚¤ãƒ³åŠ¹æœ
          'zoompan=z=\'1+0.001*on\':d=1:s=1920x1080',
          // è‰²ç›¸ã®å¾®å¦™ãªå¤‰åŒ–
          'hue=h=sin(2*PI*t/10)*30',
          // ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³åŠ¹æœ
          'fade=in:0:30'
        ])
        .outputOptions([
          '-pix_fmt yuv420p',
          '-preset fast',
          '-crf 28'
        ])
      
      // éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã¯è¿½åŠ 
      if (audioPath && fs.existsSync(audioPath)) {
        console.log('Adding audio track:', audioPath)
        ffmpegCommand
          .input(audioPath)
          .audioCodec('aac')
          .outputOptions([
            '-map 0:v:0',  // æœ€åˆã®å…¥åŠ›ã®å‹•ç”»ã‚¹ãƒˆãƒªãƒ¼ãƒ 
            '-map 1:a:0',  // äºŒç•ªç›®ã®å…¥åŠ›ã®éŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒ 
            '-shortest'    // çŸ­ã„æ–¹ã®é•·ã•ã«åˆã‚ã›ã‚‹
          ])
      } else {
        console.log('No audio track provided, generating silent video')
      }
      
      ffmpegCommand
        .output(outputPath)
        .on('start', (commandLine) => {
          console.log('FFmpeg command:', commandLine)
        })
        .on('progress', (progress) => {
          console.log('Processing: ' + progress.percent + '% done')
        })
        .on('end', () => {
          console.log('Video generation completed successfully')
          resolve(outputPath)
        })
        .on('error', (err) => {
          console.error('FFmpeg error:', err)
          reject(err)
        })
        .run()
    })
  } catch (error) {
    console.error('Canvas error:', error)
    throw error
  }
}

export async function POST(request: NextRequest) {
  try {
    console.log('å‹•ç”»ç”ŸæˆAPIé–‹å§‹:', new Date().toISOString())
    
    const body = await request.json() as VideoGenerationRequest
    console.log('ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£å—ä¿¡:', JSON.stringify(body, null, 2))
    const { audioInput, settings, transcript, customAssets } = body
    
    if (!audioInput || !settings) {
      return NextResponse.json(
        { error: 'å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™', code: 'INVALID_REQUEST' },
        { status: 400 }
      )
    }

    // 1. å¤ã„ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆ1æ™‚é–“ä»¥ä¸Šå¤ã„ã‚‚ã®ï¼‰
    try {
      const tmpDir = os.tmpdir()
      const allDirs = await fs.promises.readdir(tmpDir)
      const videoDirs = allDirs.filter(dir => dir.startsWith('video-generation-'))
      
      for (const dir of videoDirs) {
        const dirPath = path.join(tmpDir, dir)
        try {
          const stats = await fs.promises.stat(dirPath)
          const ageInHours = (Date.now() - stats.mtime.getTime()) / (1000 * 60 * 60)
          
          if (ageInHours > 1) { // 1æ™‚é–“ä»¥ä¸Šå¤ã„
            await fs.promises.rm(dirPath, { recursive: true, force: true })
            console.log('Cleaned up old directory:', dirPath)
          }
        } catch (error) {
          console.log('Failed to clean up directory:', dirPath, error)
        }
      }
    } catch (error) {
      console.log('Directory cleanup failed:', error)
    }

    // 2. éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
    const tempDir = await fs.promises.mkdtemp(path.join(os.tmpdir(), 'video-generation-'))
    console.log('Temporary directory created:', tempDir)
    
    const audioPath = await processAudioFile(audioInput, tempDir)
    console.log('Audio processing completed:', audioPath)
    
    // å®Ÿéš›ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®é•·ã•ã‚’å–å¾—
    let audioDuration: number = settings.duration // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    if (audioPath) {
      try {
        audioDuration = await getAudioDuration(audioPath)
        console.log('Using actual audio duration:', audioDuration, 'seconds')
      } catch (error) {
        console.error('Failed to get audio duration, using default:', settings.duration)
        audioDuration = settings.duration
      }
    }

    // 2. æ–‡å­—èµ·ã“ã—ã®å‡¦ç†
    let processedTranscript: TranscriptSegment[] = transcript || [
      { text: 'ã‚µãƒ³ãƒ—ãƒ«æ–‡å­—èµ·ã“ã—', startTime: 0, endTime: audioDuration }
    ]

    // 3. æ–‡å­—èµ·ã“ã—çµæœã‚’åˆ†æã—ã¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã‚·ãƒ¼ãƒ³ã‚’æŠ½å‡º
    console.log('æ–‡å­—èµ·ã“ã—åˆ†æé–‹å§‹')
    const analysisResult = await analyzeTranscriptForKeywords(processedTranscript)
    console.log('åˆ†æçµæœ:', analysisResult)

    // 4. æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åŸºã¥ã„ã¦ç´ æã‚’æ¤œç´¢
    console.log('ç´ ææ¤œç´¢é–‹å§‹')
    let videoAssets: VideoAsset[] = customAssets || []
    
    if (analysisResult.scenes.length > 0) {
      const searchedAssets = await searchMediaAssets(analysisResult.keywords, analysisResult.scenes)
      videoAssets = [...videoAssets, ...searchedAssets]
    }
    
    console.log('ç´ æå–å¾—å®Œäº†:', videoAssets.length, 'å€‹')
    
    // 5. ç´ æã‚’ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã«é…ç½®
    console.log('ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³é…ç½®é–‹å§‹')
    const timelineResult = await arrangeAssetsOnTimeline(videoAssets, audioDuration)
    console.log('ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³é…ç½®å®Œäº†:', timelineResult.timeline.length, 'å€‹ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ')

    // 5. å‹•ç”»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆãƒ¢ãƒƒã‚¯ï¼‰
    const metadata = {
      title: 'ãƒ†ã‚¹ãƒˆå‹•ç”»',
      description: 'ãƒ†ã‚¹ãƒˆç”¨ã®å‹•ç”»ã§ã™'
    }
    
    // 6. Remotionã‚’ä½¿ç”¨ã—ãŸå®Ÿéš›ã®å‹•ç”»ç”Ÿæˆ
    console.log('Remotionå‹•ç”»ç”Ÿæˆé–‹å§‹:', new Date().toISOString())
    
    let result: VideoGenerationResult
    let outputPath: string
    let fileSize: number
    
    // FFmpegã‚’ä½¿ç”¨ã—ãŸåŸºæœ¬çš„ãªå‹•ç”»ç”Ÿæˆ
  console.log('API endpoint called with:', { audioInput, settings })
  console.log('Starting FFmpeg-based video generation')
  
  try {
    // ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆï¼ˆã‚ˆã‚Šç¢ºå®Ÿãªæ–¹æ³•ï¼‰
    const tempDir = await fs.promises.mkdtemp(path.join(os.tmpdir(), 'video-generation-'))
    console.log('Created temp directory:', tempDir)
    
    // å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆæ¨©é™å•é¡Œã‚’é¿ã‘ã‚‹ãŸã‚ã€ã‚ˆã‚Šå®‰å…¨ãªãƒ‘ã‚¹ï¼‰
    outputPath = path.join(tempDir, 'output.mp4')
    console.log('Output path:', outputPath)
    
    // éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    const audioPath = await processAudioFile(audioInput, tempDir)
    console.log('processAudioFileçµæœ:', { audioPath, exists: audioPath ? fs.existsSync(audioPath) : false })
    
    // éŸ³å£°ã®æ–‡å­—èµ·ã“ã—ã‚’å®Ÿè¡Œï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãå­—å¹•ã®ãŸã‚ï¼‰
    let transcriptWithTimestamps: any[] = []
    if (audioPath && fs.existsSync(audioPath)) {
      try {
        console.log('éŸ³å£°ã®æ–‡å­—èµ·ã“ã—ã‚’é–‹å§‹...', audioPath)
        transcriptWithTimestamps = await transcribeAudioWithTimestamps(audioPath)
        console.log('æ–‡å­—èµ·ã“ã—å®Œäº†:', transcriptWithTimestamps.length, 'å€‹ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ')
      } catch (error) {
        console.error('æ–‡å­—èµ·ã“ã—ã‚¨ãƒ©ãƒ¼:', error)
        // ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ç©ºã®é…åˆ—ã‚’ä½¿ç”¨
      }
    } else {
      console.log('éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚æ–‡å­—èµ·ã“ã—ã‚’ã‚¹ã‚­ãƒƒãƒ—:', { audioPath, audioInput: typeof audioInput })
      console.log('audioInputè©³ç´°:', audioInput)
      console.log('audioInput.source:', audioInput.source, 'typeof:', typeof audioInput.source)
      
      // æ–‡å­—åˆ—å…¥åŠ›ã®å ´åˆã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’æ™‚é–“åˆ†å‰²ã—ã¦å­—å¹•ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆ
       if (typeof audioInput.source === 'string' && audioInput.source.trim()) {
         console.log('æ–‡å­—åˆ—å…¥åŠ›ã«ã‚ˆã‚‹å­—å¹•ç”Ÿæˆé–‹å§‹')
         const text = audioInput.source.trim()
        // æ—¥æœ¬èªã®å ´åˆã¯æ–‡å­—å˜ä½ã€è‹±èªã®å ´åˆã¯å˜èªå˜ä½ã§åˆ†å‰²
        const isJapanese = /[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]/.test(text)
        console.log('ãƒ†ã‚­ã‚¹ãƒˆ:', text, 'æ—¥æœ¬èªåˆ¤å®š:', isJapanese, 'æ–‡å­—ã‚³ãƒ¼ãƒ‰ç¢ºèª:', text.split('').map(c => c.charCodeAt(0)))
        
        if (isJapanese) {
          // æ—¥æœ¬èªã®å ´åˆï¼šå¥èª­ç‚¹ã§åˆ†å‰²ã—ã€é•·ã„å ´åˆã¯æ–‡å­—æ•°ã§åˆ†å‰²
          console.log('æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²é–‹å§‹:', text)
          const sentences = text.split(/[ã€‚ï¼ï¼Ÿ]/).filter(s => s.trim())
          console.log('å¥èª­ç‚¹åˆ†å‰²çµæœ:', sentences)
          const maxCharsPerSegment = 10 // ã‚ˆã‚ŠçŸ­ãè¨­å®š
          
          // å®Ÿéš›ã®éŸ³å£°ã®é•·ã•ã«åŸºã¥ã„ã¦ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ™‚é–“ã‚’è¨ˆç®—
          const totalChars = text.length
          const estimatedSegments = Math.ceil(totalChars / maxCharsPerSegment)
          const segmentDuration = audioDuration / estimatedSegments // å®Ÿéš›ã®éŸ³å£°ã®é•·ã•ã‚’ä½¿ç”¨
          
          console.log('å­—å¹•åˆ†å‰²è¨ˆç®—:', {
            totalChars,
            estimatedSegments,
            audioDuration,
            segmentDuration
          })
          
          let currentTime = 0
          
          if (sentences.length === 0 || (sentences.length === 1 && sentences[0] === text)) {
            // å¥èª­ç‚¹ãŒãªã„å ´åˆã¯æ–‡å­—æ•°ã§å¼·åˆ¶åˆ†å‰²
            console.log('å¥èª­ç‚¹ãŒãªã„ãŸã‚æ–‡å­—æ•°ã§åˆ†å‰²')
            for (let i = 0; i < text.length; i += maxCharsPerSegment) {
              const chunk = text.slice(i, i + maxCharsPerSegment)
              transcriptWithTimestamps.push({
                text: chunk,
                startTime: currentTime,
                endTime: currentTime + segmentDuration
              })
              currentTime += segmentDuration
            }
          } else {
            for (const sentence of sentences) {
              if (sentence.length <= maxCharsPerSegment) {
                transcriptWithTimestamps.push({
                  text: sentence.trim(),
                  startTime: currentTime,
                  endTime: currentTime + segmentDuration
                })
                currentTime += segmentDuration
              } else {
                // é•·ã„æ–‡ã¯åˆ†å‰²
                for (let i = 0; i < sentence.length; i += maxCharsPerSegment) {
                  const chunk = sentence.slice(i, i + maxCharsPerSegment)
                  transcriptWithTimestamps.push({
                    text: chunk,
                    startTime: currentTime,
                    endTime: currentTime + segmentDuration
                  })
                  currentTime += segmentDuration
                }
              }
            }
          }
        } else {
          // è‹±èªã®å ´åˆï¼šå˜èªå˜ä½ã§åˆ†å‰²
          const words = text.split(/\s+/)
          const wordsPerSegment = 4
          const segmentDuration = 2
          
          for (let i = 0; i < words.length; i += wordsPerSegment) {
            const segmentWords = words.slice(i, i + wordsPerSegment)
            const startTime = (i / wordsPerSegment) * segmentDuration
            const endTime = startTime + segmentDuration
            
            transcriptWithTimestamps.push({
              text: segmentWords.join(' '),
              startTime: startTime,
              endTime: endTime
            })
          }
        }
        
        console.log('ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å­—å¹•ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆ:', transcriptWithTimestamps.length, 'å€‹ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ')
      }
    }
    
    // éŸ³å£°ã‚’å­—å¹•ã¨ã—ã¦è¡¨ç¤ºã™ã‚‹å‹•ç”»ç”Ÿæˆ
    const audioText = typeof audioInput === 'string' ? audioInput : (typeof audioInput.source === 'string' ? audioInput.source : '')
    outputPath = await generateSubtitleVideo({
      audioPath: audioPath || '',
      audioInput: audioText,
      transcript: transcriptWithTimestamps.length > 0 ? transcriptWithTimestamps : (processedTranscript || []),
      settings,
      tempDir,
      timeline: timelineResult,
      audioDuration
    })
    
    // ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’å–å¾—
    const stats = await fs.promises.stat(outputPath)
    fileSize = stats.size
    
    // å…¬é–‹ç”¨ãƒ‘ã‚¹ã‚’ç”Ÿæˆ
    const publicVideoPath = `/api/video/${path.basename(outputPath)}`
    
    result = {
      videoUrl: publicVideoPath,
      thumbnailUrl: videoAssets[0]?.url || 'https://via.placeholder.com/1280x720/0066cc/ffffff?text=Generated+Video',
      duration: audioDuration,
      format: settings.format,
      size: fileSize
    }
    
    console.log('Video generation completed:', result)
    
  } catch (error) {
    console.error('Video generation error:', error)
    
    // ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒ¢ãƒƒã‚¯ã®çµæœã‚’è¿”ã™
    outputPath = '/tmp/mock-video.mp4'
    fileSize = 1024 * 1024 // 1MB
    const publicVideoPath = `/api/video/mock-video.mp4`
    
    result = {
      videoUrl: publicVideoPath,
      thumbnailUrl: videoAssets[0]?.url || 'https://via.placeholder.com/1280x720/0066cc/ffffff?text=Generated+Video',
      duration: audioDuration,
      format: settings.format,
      size: fileSize
    }
  }
    
    console.log('å®Ÿéš›ã®å‹•ç”»ç”Ÿæˆå®Œäº†:', {
      title: metadata.title,
      assets: videoAssets.length,
      outputPath,
      fileSize: `${(fileSize / 1024 / 1024).toFixed(1)}MB`
    })
    
    console.log('å‹•ç”»ç”ŸæˆAPIå®Œäº†:', new Date().toISOString())
    
    return NextResponse.json({
      success: true,
      result,
      title: metadata.title,
      description: metadata.description,
      scenesCount: 0,
      assetsUsed: videoAssets.length
    }, { status: 200 })
    
  } catch (error: any) {
    console.error('å‹•ç”»ç”Ÿæˆã‚¨ãƒ©ãƒ¼è©³ç´°:', {
      message: error.message,
      stack: error.stack,
      name: error.name,
      cause: error.cause
    })
    
    return NextResponse.json(
      {
        error: `å‹•ç”»ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: ${error.message}`,
        code: 'VIDEO_GENERATION_FAILED',
        details: error.stack,
        errorName: error.name
      },
      { status: 500 }
    )
  }
}

// å‹•ç”»ç”Ÿæˆã®é€²è¡ŒçŠ¶æ³ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®GETã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
export async function GET(request: NextRequest) {
  const { searchParams } = new URL(request.url)
  const jobId = searchParams.get('jobId')
  
  if (!jobId) {
    return NextResponse.json(
      { error: 'ã‚¸ãƒ§ãƒ–IDãŒå¿…è¦ã§ã™', code: 'INVALID_REQUEST' },
      { status: 400 }
    )
  }
  
  // å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ã‚¸ãƒ§ãƒ–ã®é€²è¡ŒçŠ¶æ³ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—
  return NextResponse.json({
    jobId,
    status: 'completed', // 'pending' | 'processing' | 'completed' | 'failed'
    progress: 100,
    message: 'å‹•ç”»ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ'
  })
}